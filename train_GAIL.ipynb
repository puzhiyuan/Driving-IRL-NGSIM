{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "import warnings\n",
    "# 忽略所有的 featurewarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NGSIM_env.envs.ngsim_env import NGSIMEnv\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "env_case_ids = [8, 9, 88, 99, 888, 999]\n",
    "\n",
    "envs = SubprocVecEnv(\n",
    "    [\n",
    "        lambda: NGSIMEnv(\n",
    "            scene=\"us-101\", period=0, vehicle_id=case_id, IDM=False\n",
    "        )\n",
    "        for case_id in env_case_ids\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "action: [0 9]\n",
      "_states: None\n",
      "rewards: -609.927714459362\n",
      "dones: True\n",
      "info: {'velocity': 2.4514807286426787, 'crashed': True, 'offroad': False, 'action': [7.314843035659861, 8.004814205270058, 5], 'features': array([3.73385735e+00, 8.17241487e-01, 3.67572733e-01, 6.20013315e-01,\n",
      "       5.22858405e-02, 3.72007598e-44, 1.00000000e+00, 0.00000000e+00,\n",
      "       5.62286343e+00]), 'time': 27}\n",
      "crashed\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "model = PPO.load(\"./models/PPO_NGSIM\")\n",
    "import torch\n",
    "# env = torch.load(\"./envs/env_9\", weights_only=False)\n",
    "\n",
    "env = NGSIMEnv(scene=\"us-101\", period=0, vehicle_id=999, IDM=False)\n",
    "torch.save(env, \"./envs/env_999\")\n",
    "\n",
    "obs = env.reset()\n",
    "for _ in range(1000):  # 运行1000个时间步\n",
    "    env.render(mode=\"rgb_array\")\n",
    "    action, _states = model.predict(obs, deterministic=True)  # 预测动作\n",
    "    print(f\"action: {action}\")\n",
    "    print(f\"_states: {_states}\")\n",
    "    obs, rewards, dones, info = env.step(action)  # 执行动作\n",
    "    print(f\"rewards: {rewards}\")\n",
    "    print(f\"dones: {dones}\")\n",
    "    print(f\"info: {info}\")\n",
    "    if info[\"crashed\"]:\n",
    "        print(\"crashed\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "# # 加载模型\n",
    "# model = PPO.load(\"./models/PPO_NGSIM\")\n",
    "\n",
    "# # 运行推理\n",
    "# obs = envs.reset()\n",
    "# for _ in range(1000):  # 运行1000个时间步\n",
    "#     envs.render(mode=\"rgb_array\")\n",
    "#     action, _states = model.predict(obs, deterministic=True)  # 预测动作\n",
    "#     print(f\"action: {action}\")\n",
    "#     print(f\"_states: {_states}\")\n",
    "#     obs, rewards, dones, info = envs.step(action)  # 执行动作\n",
    "#     print(f\"rewards: {rewards}\")\n",
    "#     print(f\"dones: {dones}\")\n",
    "#     print(f\"info: {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use DQN to train the model\n",
    "# env = NGSIMEnv(scene='us-101',period=0,vehicle_id=1200,IDM=False)\n",
    "# print(\"created env\")\n",
    "# model = DQN(\n",
    "#     \"MlpPolicy\",\n",
    "#     env,\n",
    "#     verbose=1,\n",
    "#     learning_rate=1e-3,\n",
    "#     buffer_size=50000,\n",
    "#     batch_size=64,\n",
    "# )\n",
    "# print(\"Start training\")\n",
    "# model.learn(total_timesteps=10000)\n",
    "\n",
    "# model.save(\"dqn_ngsim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\py\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 179\u001b[0m\n\u001b[0;32m    176\u001b[0m gail_trainer \u001b[38;5;241m=\u001b[39m GAIL(env_id\u001b[38;5;241m=\u001b[39menv_id, expert_data\u001b[38;5;241m=\u001b[39mexpert_data,\n\u001b[0;32m    177\u001b[0m                     total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# 开始训练（交替更新判别器和生成器）\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[43mgail_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps_per_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisc_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 143\u001b[0m, in \u001b[0;36mGAIL.train\u001b[1;34m(self, timesteps_per_iter, disc_iters, total_iters)\u001b[0m\n\u001b[0;32m    141\u001b[0m disc_loss_avg \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(disc_iters):\n\u001b[1;32m--> 143\u001b[0m     disc_loss_avg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m disc_loss_avg \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m disc_iters\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# --- 2. 使用包装器更新生成器（策略） ---\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# 这里我们将 PPO 所使用的环境替换为带有 imitation reward 的环境包装器\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 102\u001b[0m, in \u001b[0;36mGAIL.update_discriminator\u001b[1;34m(self, batch_size, disc_iters)\u001b[0m\n\u001b[0;32m    100\u001b[0m gen_obs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    101\u001b[0m gen_act \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 102\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m    104\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:64\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     63\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[env_idx]\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf()\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\py\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:94\u001b[0m, in \u001b[0;36mDummyVecEnv._save_obs\u001b[1;34m(self, env_idx, obs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 94\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuf_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[key][env_idx] \u001b[38;5;241m=\u001b[39m obs[key]\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gym import Wrapper\n",
    "\n",
    "# -------------------------------\n",
    "# 1. 定义判别器网络\n",
    "# -------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=[64, 64]):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        input_dim = obs_dim + act_dim  # 将状态和动作拼接后输入\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = h\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        layers.append(nn.Sigmoid())  # 输出在0~1之间\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        # 假设 obs: [batch, obs_dim]，act: [batch, act_dim]\n",
    "        x = torch.cat([obs, act], dim=1)\n",
    "        return self.net(x)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. 定义环境包装器，用于替换原始奖励为 imitation reward\n",
    "# -------------------------------\n",
    "class GAILRewardWrapper(Wrapper):\n",
    "    def __init__(self, env, discriminator, device='cpu'):\n",
    "        super(GAILRewardWrapper, self).__init__(env)\n",
    "        self.discriminator = discriminator\n",
    "        self.device = device\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, done, info = self.env.step(action)\n",
    "        # 将观测和动作转为 tensor（注意对动作的处理，这里假设动作为标量，转换为 float tensor）\n",
    "        obs_tensor = torch.tensor(np.array(obs), dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        # 将动作转换为 one-hot 编码（离散动作）\n",
    "        act_dim = self.action_space.n\n",
    "        action_onehot = np.zeros(act_dim, dtype=np.float32)\n",
    "        action_onehot[action] = 1.0\n",
    "        act_tensor = torch.tensor(action_onehot, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            d_val = self.discriminator(obs_tensor, act_tensor)\n",
    "        # imitation reward计算：通常使用 -log(1 - D(s,a)) 或 log(D(s,a))\n",
    "        imitation_reward = -torch.log(1 - d_val + 1e-8).item()\n",
    "        return obs, imitation_reward, done, info\n",
    "\n",
    "# -------------------------------\n",
    "# 3. GAIL 算法封装\n",
    "# -------------------------------\n",
    "class GAIL:\n",
    "    def __init__(self, env_id, expert_data, total_timesteps=100000,\n",
    "                 disc_lr=1e-3, device='cpu'):\n",
    "        \"\"\"\n",
    "        :param env_id: Gym 环境ID\n",
    "        :param expert_data: 专家演示数据，格式为 list/dict，其中每个样本包含 (obs, action)\n",
    "        :param total_timesteps: 总训练步数\n",
    "        :param disc_lr: 判别器学习率\n",
    "        :param device: 使用的设备\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.env_id = env_id\n",
    "        self.expert_data = expert_data  # 专家数据应为 list，每个元素为 (obs, action)\n",
    "        # 构建原始环境\n",
    "        self.env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "        # 获取环境观测和动作空间信息\n",
    "        obs_dim = self.env.observation_space.shape[0]\n",
    "        act_space = self.env.action_space\n",
    "        if isinstance(act_space, gym.spaces.Discrete):\n",
    "            act_dim = act_space.n\n",
    "        else:\n",
    "            act_dim = act_space.shape[0]\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        # 构建判别器\n",
    "        self.discriminator = Discriminator(obs_dim, act_dim).to(self.device)\n",
    "        self.disc_optimizer = optim.Adam(self.discriminator.parameters(), lr=disc_lr)\n",
    "        # 构建 PPO 策略（生成器）\n",
    "        # 注意：这里先用原始环境构建 PPO，后面在训练过程中，我们会用包装器替换奖励\n",
    "        self.policy = PPO(\"MlpPolicy\", self.env, verbose=1, device=self.device)\n",
    "\n",
    "    def update_discriminator(self, batch_size=64, disc_iters=10):\n",
    "        # 随机采样专家数据和生成器数据\n",
    "        expert_indices = np.random.choice(len(self.expert_data), batch_size)\n",
    "        expert_batch = [self.expert_data[i] for i in expert_indices]\n",
    "        expert_obs = np.array([sample[0] for sample in expert_batch], dtype=np.float32)\n",
    "        expert_act = np.array([sample[1] for sample in expert_batch], dtype=np.int64)\n",
    "        # 将专家动作转换为 one-hot 编码\n",
    "        expert_act_oh = np.zeros((batch_size, self.act_dim), dtype=np.float32)\n",
    "        expert_act_oh[np.arange(batch_size), expert_act] = 1.0\n",
    "\n",
    "        # 生成器数据：收集一批 trajectories\n",
    "        gen_obs = []\n",
    "        gen_act = []\n",
    "        obs = self.env.reset()\n",
    "        for _ in range(batch_size):\n",
    "            action, _ = self.policy.predict(obs, deterministic=True)\n",
    "            gen_obs.append(obs[0])\n",
    "            gen_act.append(action[0])\n",
    "            obs, _, dones, _ = self.env.step(action)\n",
    "            if dones[0]:\n",
    "                obs = self.env.reset()\n",
    "        gen_obs = np.array(gen_obs, dtype=np.float32)\n",
    "        gen_act = np.array(gen_act, dtype=np.int64)\n",
    "        gen_act_oh = np.zeros((batch_size, self.act_dim), dtype=np.float32)\n",
    "        gen_act_oh[np.arange(batch_size), gen_act] = 1.0\n",
    "\n",
    "        # 转换为 tensor\n",
    "        expert_obs_t = torch.tensor(expert_obs, dtype=torch.float32).to(self.device)\n",
    "        expert_act_t = torch.tensor(expert_act_oh, dtype=torch.float32).to(self.device)\n",
    "        gen_obs_t = torch.tensor(gen_obs, dtype=torch.float32).to(self.device)\n",
    "        gen_act_t = torch.tensor(gen_act_oh, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # 判别器输出\n",
    "        d_expert = self.discriminator(expert_obs_t, expert_act_t)\n",
    "        d_gen = self.discriminator(gen_obs_t, gen_act_t)\n",
    "\n",
    "        # 判别器损失：二分类交叉熵\n",
    "        loss_expert = -torch.log(d_expert + 1e-8).mean()\n",
    "        loss_gen = -torch.log(1 - d_gen + 1e-8).mean()\n",
    "        disc_loss = loss_expert + loss_gen\n",
    "\n",
    "        self.disc_optimizer.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        self.disc_optimizer.step()\n",
    "        return disc_loss.item()\n",
    "\n",
    "    def train(self, timesteps_per_iter=1000, disc_iters=10, total_iters=100):\n",
    "        \"\"\"\n",
    "        交替更新判别器和生成器\n",
    "        \"\"\"\n",
    "        for it in range(total_iters):\n",
    "            # --- 1. 更新判别器 ---\n",
    "            disc_loss_avg = 0\n",
    "            for _ in range(disc_iters):\n",
    "                disc_loss_avg += self.update_discriminator()\n",
    "            disc_loss_avg /= disc_iters\n",
    "\n",
    "            # --- 2. 使用包装器更新生成器（策略） ---\n",
    "            # 这里我们将 PPO 所使用的环境替换为带有 imitation reward 的环境包装器\n",
    "            wrapped_env = DummyVecEnv([lambda: GAILRewardWrapper(gym.make(self.env_id),\n",
    "                                                                   self.discriminator,\n",
    "                                                                   device=self.device)])\n",
    "            self.policy.set_env(wrapped_env)\n",
    "            # 更新策略：这里调用 learn()，注意 timesteps_per_iter 可根据需要调整\n",
    "            self.policy.learn(total_timesteps=timesteps_per_iter)\n",
    "            \n",
    "            print(f\"Iter {it+1}/{total_iters}: Disc Loss = {disc_loss_avg:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4. 使用示例\n",
    "# -------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # 假设 expert_data 是提前准备好的专家演示数据，\n",
    "    # 格式为 list，每个元素为 (obs, action)。\n",
    "    # 这里为了演示，我们随机生成一些数据（实际应用中应加载真实专家数据）\n",
    "    env_id = \"CartPole-v1\"\n",
    "    env = gym.make(env_id)\n",
    "    expert_data = []\n",
    "    for _ in range(1000):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            expert_data.append((obs, action))\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            # observation, reward, terminated, truncated, info\n",
    "    # 创建 GAIL 算法实例\n",
    "    gail_trainer = GAIL(env_id=env_id, expert_data=expert_data,\n",
    "                        total_timesteps=100000, device='cpu')\n",
    "    # 开始训练（交替更新判别器和生成器）\n",
    "    gail_trainer.train(timesteps_per_iter=1000, disc_iters=10, total_iters=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
